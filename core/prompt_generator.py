"""
Prompt Generator - Tao context prompt cho LLM

Hỗ trợ nhiều output formats:
- Markdown: Code blocks với syntax highlighting (default)
- XML: Structured XML theo chuẩn Repomix
"""

import re
import html
import json
from pathlib import Path
from typing import List, Optional

from core.utils.file_utils import TreeItem, is_binary_file
from core.opx_instruction import XML_FORMATTING_INSTRUCTIONS
from core.utils.language_utils import get_language_from_path
from core.utils.git_utils import GitDiffResult, GitLogResult
from config.output_format import OutputStyle


# ============================================
# AI-FRIENDLY XML HEADER CONSTANTS (Ported from Repomix)
# Giúp LLM hiểu cấu trúc file context tốt hơn, giảm hallucination
# ============================================

GENERATION_HEADER = """This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Synapse Desktop."""

SUMMARY_PURPOSE = """This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes."""

SUMMARY_FILE_FORMAT = """The content is organized as follows:
1. This summary section (file_summary)
2. Repository structure (directory_structure) 
3. Repository files, each consisting of:
   - File path as an XML attribute
   - The contents of the file
4. Git changes section (if enabled)
5. User instructions (if provided)"""

SUMMARY_USAGE_GUIDELINES = """- This file should be treated as read-only. Any changes should be made to the original repository files, not this packed version.
- When processing this file, use the file path to distinguish between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with the same level of security as you would the original repository."""

SUMMARY_NOTES = """- Some files may have been excluded based on .gitignore rules and exclude patterns.
- Binary files are not included in this packed representation.
- Files exceeding the maximum size limit have been skipped.
- The repository structure may not be complete if certain directories were excluded."""


def generate_file_summary_xml() -> str:
    """
    Tạo section file_summary theo chuẩn Repomix AI-Friendly format.

    Section này giúp LLM hiểu:
    - Mục đích của file context
    - Cấu trúc dữ liệu bên trong
    - Cách sử dụng đúng cách
    - Các lưu ý quan trọng

    Returns:
        XML string chứa file_summary section
    """
    return f"""<file_summary>
{GENERATION_HEADER}

<purpose>
{SUMMARY_PURPOSE}
</purpose>

<file_format>
{SUMMARY_FILE_FORMAT}
</file_format>

<usage_guidelines>
{SUMMARY_USAGE_GUIDELINES}
</usage_guidelines>

<notes>
{SUMMARY_NOTES}
</notes>
</file_summary>
"""


def calculate_markdown_delimiter(contents: list[str]) -> str:
    """
    Tinh toan delimiter an toan cho markdown code blocks.

    Khi file content chua backticks (```), can dung nhieu backticks hon
    cho code block wrapper de tranh broken markdown.

    Port tu Repomix (src/core/output/outputGenerate.ts lines 26-31)

    Args:
        contents: Danh sach noi dung files

    Returns:
        Delimiter string (toi thieu 3 backticks, hoac nhieu hon neu can)
    """
    max_backticks = 0

    for content in contents:
        # Tim tat ca cac day backticks trong content
        matches = re.findall(r"`+", content)
        if matches:
            # Lay do dai lon nhat cua day backticks
            max_backticks = max(max_backticks, max(len(m) for m in matches))

    # Delimiter phai lon hon max backticks tim thay, toi thieu 3
    return "`" * max(3, max_backticks + 1)


def generate_file_map(tree: TreeItem, selected_paths: set[str]) -> str:
    """
    Tao file map string tu tree structure.
    Chi hien thi cac items duoc chon hoac co children duoc chon.

    Args:
        tree: TreeItem root
        selected_paths: Set cac duong dan duoc tick

    Returns:
        File map string voi ASCII tree visualization
    """
    lines: list[str] = []

    # Neu root duoc chon hoac co descendants duoc chon
    if _has_selected_descendant(tree, selected_paths):
        lines.append(tree.path)  # Root path

        # Filter children
        filtered_children = _filter_selected_tree(tree.children, selected_paths)

        if filtered_children:
            _build_tree_string(filtered_children, "", lines)

    return "\n".join(lines)


def _has_selected_descendant(item: TreeItem, selected_paths: set[str]) -> bool:
    """Kiem tra item hoac descendants co duoc chon khong"""
    if item.path in selected_paths:
        return True

    for child in item.children:
        if _has_selected_descendant(child, selected_paths):
            return True

    return False


def _filter_selected_tree(
    items: list[TreeItem], selected_paths: set[str]
) -> list[TreeItem]:
    """Loc chi giu lai cac items duoc chon hoac co descendants duoc chon"""
    result: list[TreeItem] = []

    for item in items:
        is_selected = item.path in selected_paths
        has_selected_children = any(
            _has_selected_descendant(child, selected_paths) for child in item.children
        )

        if is_selected or has_selected_children:
            # Tao copy voi filtered children
            filtered_item = TreeItem(
                label=item.label,
                path=item.path,
                is_dir=item.is_dir,
                children=(
                    _filter_selected_tree(item.children, selected_paths)
                    if item.children
                    else []
                ),
            )
            result.append(filtered_item)

    return result


def _build_tree_string(items: list[TreeItem], prefix: str, lines: list[str]) -> None:
    """Xay dung ASCII tree string voi connectors (├──, └──, │)"""
    for i, item in enumerate(items):
        is_last = i == len(items) - 1
        # Connector: └── cho item cuoi, ├── cho cac item khac
        connector = "└── " if is_last else "├── "
        lines.append(f"{prefix}{connector}{item.label}")

        if item.children:
            # Prefix moi: "    " neu la item cuoi (khong can duong doc), "│   " neu con item khac
            new_prefix = prefix + ("    " if is_last else "│   ")
            _build_tree_string(item.children, new_prefix, lines)


def generate_file_contents(
    selected_paths: set[str], max_file_size: int = 1024 * 1024
) -> str:
    """
    Tao file contents string cho cac files duoc chon.

    Su dung Smart Markdown Delimiter de tranh broken markdown
    khi file content chua backticks.

    PERFORMANCE OPTIMIZATIONS:
    - Pre-allocate list capacity hint
    - Use StringIO for large outputs
    - Batch file reads
    - Early exit on cancellation

    Args:
        selected_paths: Set cac duong dan file duoc tick
        max_file_size: Maximum file size to include (default 1MB)

    Returns:
        File contents string voi markdown code blocks
    """
    from io import StringIO
    
    # Sort paths de thu tu nhat quan
    sorted_paths = sorted(selected_paths)
    
    if not sorted_paths:
        return ""

    # Phase 1: Doc tat ca file contents truoc de tinh delimiter
    # Pre-allocate with capacity hint
    file_data: list[tuple[Path, str | None, str | None]] = []
    all_contents: list[str] = []
    max_backticks = 3  # Track max backticks for delimiter calculation

    for path_str in sorted_paths:
        path = Path(path_str)

        try:
            # Chi xu ly files
            if not path.is_file():
                continue

            # Skip binary files (check magic bytes)
            if is_binary_file(path):
                file_data.append((path, None, "Binary file"))
                continue

            # Skip files that are too large
            try:
                file_size = path.stat().st_size
                if file_size > max_file_size:
                    file_data.append(
                        (path, None, f"File too large ({file_size // 1024}KB)")
                    )
                    continue
                # Skip empty files
                if file_size == 0:
                    file_data.append((path, "", None))
                    continue
            except OSError:
                pass

            # Doc content
            content = path.read_text(encoding="utf-8", errors="replace")
            file_data.append((path, content, None))
            
            # Track backticks for delimiter (inline calculation)
            if '`' in content:
                import re
                matches = re.findall(r"`+", content)
                if matches:
                    max_backticks = max(max_backticks, max(len(m) for m in matches) + 1)

        except (OSError, IOError) as e:
            file_data.append((path, None, f"Error reading file: {e}"))

    # Phase 2: Delimiter is already calculated inline
    delimiter = "`" * max(3, max_backticks)

    # Phase 3: Generate output using StringIO for efficiency
    output = StringIO()
    first = True

    for path, content, error in file_data:
        if not first:
            output.write("\n")
        first = False
        
        if error:
            output.write(f"File: {path}\n*** Skipped: {error} ***\n")
        elif content is not None:
            language = get_language_from_path(str(path))
            output.write(f"File: {path}\n{delimiter}{language}\n{content}\n{delimiter}\n")

    return output.getvalue().strip()


def generate_file_contents_xml(
    selected_paths: set[str], max_file_size: int = 1024 * 1024
) -> str:
    """
    Tạo file contents theo Repomix XML format.

    Output format:
    <files>
      <file path="src/main.py">
        content here
      </file>
    </files>

    Args:
        selected_paths: Set các đường dẫn file được tick
        max_file_size: Maximum file size to include (default 1MB)

    Returns:
        File contents string với XML structure
    """
    sorted_paths = sorted(selected_paths)
    file_elements: list[str] = []

    for path_str in sorted_paths:
        path = Path(path_str)

        try:
            if not path.is_file():
                continue

            # Skip binary files (check magic bytes, not just extension)
            if is_binary_file(path):
                file_elements.append(
                    f'<file path="{html.escape(str(path))}" skipped="true">Binary file</file>'
                )
                continue

            # Skip files that are too large
            try:
                file_size = path.stat().st_size
                if file_size > max_file_size:
                    file_elements.append(
                        f'<file path="{html.escape(str(path))}" skipped="true">File too large ({file_size // 1024}KB)</file>'
                    )
                    continue
            except OSError:
                pass

            # Read and escape content for XML
            content = path.read_text(encoding="utf-8", errors="replace")
            # Escape XML special characters trong content
            escaped_content = html.escape(content)
            file_elements.append(
                f'<file path="{html.escape(str(path))}">\n{escaped_content}\n</file>'
            )

        except (OSError, IOError) as e:
            file_elements.append(
                f'<file path="{html.escape(str(path))}" skipped="true">Error: {html.escape(str(e))}</file>'
            )

    if not file_elements:
        return "<files></files>"

    return "<files>\n" + "\n".join(file_elements) + "\n</files>"


def generate_file_contents_json(
    selected_paths: set[str], max_file_size: int = 1024 * 1024
) -> str:
    """
    Tạo file contents theo JSON format.

    Output format (serialized JSON string):
    {
        "path/to/file": "content",
        "path/to/another": "content"
    }

    Args:
        selected_paths: Set các đường dẫn file được tick
        max_file_size: Maximum file size to include (default 1MB)

    Returns:
        JSON string containing file paths and contents
    """
    sorted_paths = sorted(selected_paths)
    files_dict = {}

    for path_str in sorted_paths:
        path = Path(path_str)

        try:
            if not path.is_file():
                continue

            # Skip binary files (check magic bytes, not just extension)
            if is_binary_file(path):
                files_dict[str(path)] = "Binary file (skipped)"
                continue

            # Skip files that are too large
            try:
                file_size = path.stat().st_size
                if file_size > max_file_size:
                    files_dict[str(path)] = (
                        f"File too large ({file_size // 1024}KB) (skipped)"
                    )
                    continue
            except OSError:
                pass

            # Read content
            content = path.read_text(encoding="utf-8", errors="replace")
            files_dict[str(path)] = content

        except (OSError, IOError) as e:
            files_dict[str(path)] = f"Error reading file: {e}"

    return json.dumps(files_dict, ensure_ascii=False)


def generate_file_contents_plain(
    selected_paths: set[str], max_file_size: int = 1024 * 1024
) -> str:
    """
    Tạo file contents theo định dạng Plain Text.

    Format:
    File: path/to/file
    ----------------
    content
    ----------------

    Args:
        selected_paths: Set các đường dẫn file được tick
        max_file_size: Maximum file size to include (default 1MB)

    Returns:
        String containing file paths and contents in plain text format
    """
    sorted_paths = sorted(selected_paths)
    file_elements = []

    separator = "-" * 16

    for path_str in sorted_paths:
        path = Path(path_str)

        try:
            if not path.is_file():
                continue

            # Header cho mỗi file
            file_header = f"File: {path_str}\n{separator}"

            # Content handling
            content_display = ""

            # Skip binary files (check magic bytes, not just extension)
            if is_binary_file(path):
                content_display = "Binary file (skipped)"
            else:
                # Skip files that are too large
                try:
                    file_size = path.stat().st_size
                    if file_size > max_file_size:
                        content_display = (
                            f"File too large ({file_size // 1024}KB) (skipped)"
                        )
                    else:
                        # Read content
                        content_display = path.read_text(
                            encoding="utf-8", errors="replace"
                        ).strip()
                except OSError:
                    pass

            file_elements.append(f"{file_header}\n{content_display}\n{separator}")

        except (OSError, IOError) as e:
            file_elements.append(
                f"File: {path_str}\n{separator}\nError reading file: {e}\n{separator}"
            )

    if not file_elements:
        return "No files selected."

    return "\n\n".join(file_elements)


def generate_smart_context(
    selected_paths: set[str], 
    max_file_size: int = 1024 * 1024,
    include_relationships: bool = False
) -> str:
    """
    Tao Smart Context string - chi chua code structure (signatures, docstrings).
    Dung Tree-sitter de parse va trich xuat cau truc thay vi raw content.

    Su dung Smart Markdown Delimiter de tranh broken markdown
    khi file content chua backticks.
    
    OPTIMIZATION: Parallel processing khi có >5 files.

    Args:
        selected_paths: Set cac duong dan file duoc tick
        max_file_size: Maximum file size to include (default 1MB)
        include_relationships: Nếu True, append relationships section (CodeMaps)

    Returns:
        Smart context string voi code signatures
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from core.smart_context import smart_parse, is_supported

    sorted_paths = sorted(selected_paths)
    
    def _process_single_file(path_str: str) -> tuple[Path, str | None, str | None]:
        """
        Process một file và return (path, smart_content, error).
        Helper function cho parallel processing.
        """
        path = Path(path_str)
        
        try:
            if not path.is_file():
                return (path, None, "Not a file")
            
            # Skip binary files (check magic bytes, not just extension)
            if is_binary_file(path):
                return (path, None, "Binary file")
            
            # Skip files that are too large
            try:
                file_size = path.stat().st_size
                if file_size > max_file_size:
                    return (path, None, f"File too large ({file_size // 1024}KB)")
            except OSError:
                pass
            
            # Doc raw content
            raw_content = path.read_text(encoding="utf-8", errors="replace")
            
            # Kiem tra ho tro Smart Context
            ext = path.suffix.lstrip(".")
            if not is_supported(ext):
                return (path, None, f"Smart Context not available for .{ext} files")
            
            # Try Smart Parse với relationships nếu enabled
            smart_content = smart_parse(
                path_str, raw_content, include_relationships=include_relationships
            )
            
            if smart_content:
                return (path, smart_content, None)
            else:
                return (path, None, "Smart Context parse failed")
                
        except (OSError, IOError) as e:
            return (path, None, f"Error reading file: {e}")
    
    # Phase 1: Process files (parallel nếu >5 files, sequential nếu ít)
    file_data: list[tuple[Path, str | None, str | None]] = []
    all_contents: list[str] = []
    
    if len(sorted_paths) > 5:
        # PARALLEL processing với ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=min(8, len(sorted_paths))) as executor:
            futures = {executor.submit(_process_single_file, p): p for p in sorted_paths}
            for future in as_completed(futures):
                result = future.result()
                file_data.append(result)
                if result[1]:  # smart_content exists
                    all_contents.append(result[1])
        # Sort lại theo path để maintain order
        file_data.sort(key=lambda x: str(x[0]))
    else:
        # Sequential processing cho ít files
        for path_str in sorted_paths:
            result = _process_single_file(path_str)
            file_data.append(result)
            if result[1]:
                all_contents.append(result[1])

    # Phase 2: Tinh Smart Markdown Delimiter
    delimiter = calculate_markdown_delimiter(all_contents)

    # Phase 3: Generate output voi dynamic delimiter
    contents: list[str] = []
    contents_append = contents.append

    for path, smart_content, error in file_data:
        if error:
            contents_append(f"File: {path}\n*** Skipped: {error} ***\n")
        elif smart_content is not None:
            language = get_language_from_path(str(path))
            contents_append(
                f"File: {path} [Smart Context]\n{delimiter}{language}\n{smart_content}\n{delimiter}\n"
            )

    return "\n".join(contents).strip()


def generate_prompt(
    file_map: str,
    file_contents: str,
    user_instructions: str = "",
    include_xml_formatting: bool = False,
    git_diffs: Optional[GitDiffResult] = None,
    git_logs: Optional[GitLogResult] = None,
    output_style: OutputStyle = OutputStyle.XML,
) -> str:
    """
    Tao prompt hoan chinh de gui cho LLM.

    Args:
        file_map: File map string tu generate_file_map()
        file_contents: File contents string tu generate_file_contents() hoac generate_file_contents_xml()
        user_instructions: Huong dan tu nguoi dung
        include_xml_formatting: Co bao gom OPX instructions khong
        git_diffs: Optional git diffs (work tree & staged)
        git_logs: Optional git logs
        output_style: Dinh dang dau ra (MARKDOWN hoac XML)

    Returns:
        Prompt hoan chinh
    """
    # Tao prompt structure dua vao output_style
    if output_style == OutputStyle.XML:
        # XML format - structured tags với AI-Friendly header
        # Bổ sung file_summary section theo chuẩn Repomix
        file_summary = generate_file_summary_xml()
        prompt = f"""{file_summary}
<directory_structure>
{file_map}
</directory_structure>

{file_contents}
"""
    elif output_style == OutputStyle.JSON:
        # JSON format
        # file_contents is already a JSON string of files dict
        # We construct a full JSON object including metadata
        try:
            files_data = json.loads(file_contents)
        except json.JSONDecodeError:
            files_data = {}

        prompt_data = {
            "directory_structure": file_map,
            "files": files_data,
        }

        if user_instructions:
            prompt_data["instructions"] = user_instructions

        # Git data
        if git_diffs:
            prompt_data["git_diffs"] = {
                "work_tree": git_diffs.work_tree_diff,
                "staged": git_diffs.staged_diff,
            }

        if git_logs:
            prompt_data["git_logs"] = git_logs.log_content

        if include_xml_formatting:
            # For JSON, we might want a different field or explanation,
            # but let's keep it as text instruction for now if requested
            prompt_data["formatting_instructions"] = XML_FORMATTING_INSTRUCTIONS

        return json.dumps(prompt_data, ensure_ascii=False, indent=2)

    elif output_style == OutputStyle.PLAIN:
        # Plain text format
        # No XML tags, just concatenated data
        prompt_parts = []

        if user_instructions:
            prompt_parts.append(f"Instructions:\n{user_instructions}")
            prompt_parts.append("-" * 32)

        prompt_parts.append(f"Directory Structure:\n{file_map}")
        prompt_parts.append("-" * 32)

        prompt_parts.append(f"File Contents:\n{file_contents}")

        # Git data
        if git_diffs:
            prompt_parts.append("-" * 32)
            prompt_parts.append(
                f"Git Diffs:\nWork Tree:\n{git_diffs.work_tree_diff}\n\nStaged:\n{git_diffs.staged_diff}"
            )

        if git_logs:
            prompt_parts.append("-" * 32)
            prompt_parts.append(f"Git Logs:\n{git_logs.log_content}")

        return "\n\n".join(prompt_parts)

    else:
        # Markdown format (default) - original behavior
        prompt = f"""<file_map>
{file_map}
</file_map>

<file_contents>
{file_contents}
</file_contents>
"""

    # Add Git Changes section
    if git_diffs or git_logs:
        prompt += "\n<git_changes>\n"

        if git_diffs:
            if git_diffs.work_tree_diff:
                prompt += f"<git_diff_worktree>\n{git_diffs.work_tree_diff}\n</git_diff_worktree>\n"
            if git_diffs.staged_diff:
                prompt += (
                    f"<git_diff_staged>\n{git_diffs.staged_diff}\n</git_diff_staged>\n"
                )

        if git_logs and git_logs.log_content:
            prompt += f"<git_log>\n{git_logs.log_content}\n</git_log>\n"

        prompt += "</git_changes>\n"

    if include_xml_formatting:
        prompt += f"\n{XML_FORMATTING_INSTRUCTIONS}"

    if user_instructions and user_instructions.strip():
        prompt += f"\n<user_instructions>\n{user_instructions.strip()}\n</user_instructions>\n"

    return prompt
